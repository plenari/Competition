{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 线下\n",
    "    1.词向量，\n",
    "        1.1 分词，训练接下来呢？\n",
    "    2. 句向量\n",
    "        2.1 分词，训练句子。\n",
    "\n",
    "### 2.线上\n",
    "    1.初赛只需要分词就可以了。\n",
    "    2.复赛    \n",
    "      2.1 如果线上数据量变大，可能需要先用所有的单词初始化，然后单步训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read file\n",
    "def read1():\n",
    "    '''\n",
    "    读取并保存第一次\n",
    "    '''\n",
    "    file1=r'../data/atec_nlp_sim_train.csv'\n",
    "    file2=r'../data/atec_nlp_sim_train_add.csv'\n",
    "    t1=pd.read_csv(file1,index_col=None,header=None,encoding='utf-8',sep='\\t')\n",
    "    t2=pd.read_csv(file2,index_col=None,header=None,encoding='utf-8',sep='\\t')\n",
    "    train=pd.concat([t1,t2],axis=0)\n",
    "    return train\n",
    "#read1().to_csv('../data/nlp_train.csv',sep='\\t',encoding='utf-8',header=None,index=None)\n",
    "\n",
    "def read2():\n",
    "    '''\n",
    "    第一次读后的结果保存之后在读取\n",
    "    '''\n",
    "    file=r'../data/nlp_train.csv'\n",
    "    train=pd.read_csv(file,sep='\\t',encoding='utf-8',header=None,index_col=None)\n",
    "    return train\n",
    "train=read2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.1 字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_append(df):\n",
    "    '''\n",
    "    把训练集的字符串结合起来。\n",
    "    **莫名其妙出来一个\\ufeff\n",
    "    '''\n",
    "    str_append=df.loc[:,1].append(df.loc[:,2]).reset_index(drop=True)\n",
    "    return str_append.values\n",
    "sentence=sent_append(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.2分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 包含词性的分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jieba import posseg \n",
    "def cut_pseg(sentences):\n",
    "    '''包含词性的分词\n",
    "    '''\n",
    "    jieba.add_word('花呗')\n",
    "    jieba.add_word('借呗')\n",
    "    sent_cut=[list(posseg.cut(i)) for i in sentences]\n",
    "    word=[[j.word for j in i] for i in sent_cut]\n",
    "    flag=[[j.flag for j in i] for i in sent_cut]\n",
    "    return word,flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 普通的分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\customer\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.604 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def cut_sent(sentences):\n",
    "    '''分词，用空格隔开\n",
    "    '''\n",
    "    jieba.add_word('花呗')\n",
    "    jieba.add_word('借呗')\n",
    "    sent_cut=[' '.join(jieba.cut(i,HMM=True)) for i in sentences]\n",
    "    return sent_cut\n",
    "sent_cut=cut_sent(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把分词后的数据写入到文档中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff 怎么 更改 花呗 手机号码'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_cut[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_sentences(sent):\n",
    "    '''\n",
    "    把数据写入文档中,需要先分词\n",
    "    '''    \n",
    "    with open('../data/sentences.txt','w',encoding='utf-8') as f:        \n",
    "        for i in sent:\n",
    "            f.write(i+'\\n')\n",
    "#write_sentences(sent_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.3 词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=word2vec.Word2Vec(sent_cut,min_count=1,iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../model/model')\n",
    "model=word2vec.Word2Vec.load('../model/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 利用以上的信息训练网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  一般特征\n",
    "句子长度，\n",
    "* 多个版本的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 文档和文中对应的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 句向量\n",
    "* doc2vec [参考](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Doc2Vec in module gensim.models.doc2vec:\n",
      "\n",
      "class Doc2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Class for training, using and evaluating neural networks described in http://arxiv.org/pdf/1405.4053v2.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Doc2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tag)\n",
      " |  \n",
      " |  __init__(self, documents=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)\n",
      " |      Initialize the model from an iterable of `documents`. Each document is a\n",
      " |      TaggedDocument object that will be used for training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterables\n",
      " |          The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          If you don't supply `documents`, the model is left uninitialized -- use if\n",
      " |          you plan to initialize it in some other way.\n",
      " |      \n",
      " |      dm : int {1,0}\n",
      " |          Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
      " |          Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
      " |      \n",
      " |      size : int\n",
      " |          Dimensionality of the feature vectors.\n",
      " |      window : int\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      alpha : float\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      min_count : int\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      max_vocab_size : int\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      workers : int\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      iter : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      hs : int {1,0}\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      dm_mean : int {1,0}\n",
      " |          If 0 , use the sum of the context word vectors. If 1, use the mean.\n",
      " |          Only applies when `dm` is used in non-concatenative mode.\n",
      " |      dm_concat : int {1,0}\n",
      " |          If 1, use concatenation of context vectors rather than sum/average;\n",
      " |          Note concatenation results in a much-larger model, as the input\n",
      " |          is no longer the size of one (sampled or arithmetically combined) word vector, but the\n",
      " |          size of the tag(s) and all words in the context strung together.\n",
      " |      dm_tag_count : int\n",
      " |          Expected constant number of document tags per document, when using\n",
      " |          dm_concat mode; default is 1.\n",
      " |      dbow_words : int {1,0}\n",
      " |          If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\n",
      " |          doc-vector training; If 0, only trains doc-vectors (faster).\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Abbreviated name reflecting major configuration paramaters.\n",
      " |  \n",
      " |  build_vocab(self, documents, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence is a iterable of iterables (can simply be a list of unicode strings too).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterables\n",
      " |          The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      " |          in :mod:`~gensim.models.doc2vec` module for such examples.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      progress_per : int\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      Build model vocabulary from a passed dictionary that contains (word,word count).\n",
      " |      Words must be of type unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict\n",
      " |          Word,Word_Count dictionary.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      corpus_count : int\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |          of the model.\n",
      " |      update : bool\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from gensim.models.word2vec import Word2Vec\n",
      " |      >>> model= Word2Vec()\n",
      " |      >>> model.build_vocab_from_freq({\"Word1\": 15, \"Word2\": 20})\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |  \n",
      " |  delete_temporary_training_data(self, keep_doctags_vectors=True, keep_inference=True)\n",
      " |      Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keep_doctags_vectors : bool\n",
      " |          Set `keep_doctags_vectors` to False if you don't want to save doctags vectors,\n",
      " |          in this case you can't to use docvecs's most_similar, similarity etc. methods.\n",
      " |      keep_inference : bool\n",
      " |          Set `keep_inference` to False if you don't want to store parameters that is used for infer_vector method\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings.\n",
      " |  \n",
      " |  estimated_lookup_memory(self)\n",
      " |      Estimated memory for tag lookup; 0 if using pure int tags.\n",
      " |  \n",
      " |  infer_vector(self, doc_words, alpha=0.1, min_alpha=0.0001, steps=5)\n",
      " |      Infer a vector for given post-bulk training document.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc_words : :obj: `list` of :obj: `str`\n",
      " |          Document should be a list of (word) tokens.\n",
      " |      alpha : float\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      steps : int\n",
      " |          Number of times to train the new document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :obj: `numpy.ndarray`\n",
      " |          Returns the inferred vector for the new document.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      If `replace` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |      \n",
      " |      Note that you **cannot continue training or inference** after doing a replace.\n",
      " |      The model becomes effectively read-only = you can call `most_similar`, `similarity`\n",
      " |      etc., but not `train` or `infer_vector`.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Reuse shareable structures from other_model.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, doctag_vec=False, word_vec=True, prefix='*dt_', fvocab=None, binary=False)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in.\n",
      " |      doctag_vec : bool\n",
      " |          Indicates whether to store document vectors.\n",
      " |      word_vec : bool\n",
      " |          Indicates whether to store word vectors.\n",
      " |      prefix : str\n",
      " |          Uniquely identifies doctags from word vocab, and avoids collision\n",
      " |          in case of repeated string in doctag and word vocab.\n",
      " |      fvocab : str\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool\n",
      " |          If True, the data wil be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |  \n",
      " |  train(self, documents, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
      " |      The `documents` iterable can be simply a list of TaggedDocument elements.\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) alpha to min_alpha, and accurate\n",
      " |      progress-percentage logging, either total_examples (count of sentences) or total_words (count of\n",
      " |      raw words in sentences) **MUST** be provided (if the corpus is the same as was provided to\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.build_vocab()`, the count of examples in that corpus\n",
      " |      will be available in the model's :attr:`corpus_count` property).\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case,\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train()` is only called once,\n",
      " |      the model's cached `iter` value should be supplied as `epochs` value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterables\n",
      " |          The `documents` iterable can be simply a list of TaggedDocument elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      " |          in :mod:`~gensim.models.doc2vec` module for such examples.\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in documents.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float\n",
      " |          Initial learning rate.\n",
      " |      end_alpha : float\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |      word_count : int\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved object (using :meth:`~gensim.utils.SaveLoad.save`) from file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IOError\n",
      " |          When methods are called on instance (should be called from class).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  dbow\n",
      " |      int {1,0} : `dbow=1` indicates `distributed bag of words` (PV-DBOW) else\n",
      " |      'distributed memory' (PV-DM) is used.\n",
      " |  \n",
      " |  dm\n",
      " |      int {1,0} : `dm=1` indicates 'distributed memory' (PV-DM) else\n",
      " |      `distributed bag of words` (PV-DBOW) is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated. Use self.wv.doesnt_match() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated. Use self.wv.evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated. Use self.wv.most_similar() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated. Use self.wv.most_similar_cosmul() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated. Use self.wv.n_similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_vector() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_word() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated. Use self.wv.similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated. Use self.wv.wmdistance() instead.\n",
      " |      Refer to the documentation for `gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseAny2VecModel:\n",
      " |  \n",
      " |  save(self, fname_or_handle, **kwargs)\n",
      " |      Save the object to file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None -  automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This avoids pickle memory errors and allows mmap'ing large arrays\n",
      " |          back on load efficiently.\n",
      " |          If list of str - this attributes will be stored in separate files, the automatic check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int\n",
      " |          Limit for automatic separation.\n",
      " |      ignore : frozenset of str\n",
      " |          Attributes that shouldn't be serialize/store.\n",
      " |      pickle_protocol : int\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.models.Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 直接读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = gensim.models.doc2vec.TaggedLineDocument('../data/sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_doc=gensim.models.Doc2Vec(documents,epochs=10000,window=4,sample=1e-5,workers=20,min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name='../model/model_doc2vec'\n",
    "doc2vec_doc.save(doc_name)\n",
    "#doc2vec_doc = gensim.models.Doc2Vec.load(doc_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 如果不想训练，可以删掉训练文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.delete_temporary_training_data(keep_doctags_vectors=True, \\\n",
    "#                                     keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算句子的向量\n",
    "#test_vec=doc2vec_doc.infer_vector(['借呗', '换网', '商贷'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121605 0.6982710361480713 花呗 忘记 密码\n",
      "93559 0.6922953724861145 忘记 还 花呗\n",
      "32808 0.6873430609703064 花呗 密码 忘记\n",
      "143742 0.6861186027526855 忘记 花呗 密码\n",
      "136644 0.6854676008224487 花呗 忘记 密码\n"
     ]
    }
   ],
   "source": [
    "test_vec=doc2vec_doc.infer_vector([ '花呗', '账号','忘记'])\n",
    "#计算与内容类似的文字\n",
    "for i,j in doc2vec_doc.docvecs.most_similar([test_vec],topn=5):\n",
    "    print(i,j,sent_cut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_sentences():\n",
    "    '''\n",
    "    读取分割好的句子\n",
    "    '''\n",
    "    file='../data/sentences.txt'\n",
    "    sentences=pd.read_table(file,encoding='utf-8',header=None)\n",
    "    return sentences\n",
    "sentences=read_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c432eeaa8cef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc2vec_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msimilar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc2vec_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mrank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimilar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mranks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ranks=[]\n",
    "second=[]\n",
    "for tags,words in enumerate(sentences.values):\n",
    "    vec=doc2vec_doc.infer_vector(words)\n",
    "    similar=doc2vec_doc.docvecs.most_similar([vec],topn=len(sentences))\n",
    "    rank=[doc for doc, sim in similar].index(tags)\n",
    "    ranks.append(rank)\n",
    "    second.append(similar[1])\n",
    "    if tags%10000==0:\n",
    "        print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* doc2vec准确率判断\n",
    "* 验证句子训练的效果。看看与自己最相似的是不是自己\n",
    "* 看一下计算出来自己排在哪一个名词，如果是0自己与自己相似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 自己构造tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造自己想要的\n",
    "train_corpus=[]\n",
    "for tags,words in enumerate(sent_cut):\n",
    "    train_corpus.append(gensim.models.doc2vec.TaggedDocument(words,[tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docvec=gensim.models.Doc2Vec(train_corpus,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 关键字提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hb='为何我无法申请开通花呗信用卡收款'\n",
    "def extra_tags_by_text(text,*arg):\n",
    "    '''\n",
    "    提取关键字并按照文章顺序返回。\n",
    "    不能使用带权重和标签的返回值。 \n",
    "    '''\n",
    "    extra=jieba.analyse.extract_tags(text,*arg)\n",
    "    index=[text.find(i) for i in extra ]\n",
    "    index=np.argsort(index)\n",
    "    extra=[extra[i] for i in index]\n",
    "    return extra\n",
    "print(extra_tags_by_text(hb))\n",
    "hb2=extra_tags_by_text(hb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
