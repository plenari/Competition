{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Tensor\n",
    "* 对参数优化，需要对参数求导\n",
    "* 自变量和因变量不需要求导？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2325,  0.9943,  0.7774],\n",
       "         [ 0.6296,  0.6930,  0.7369]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module torch.autograd.variable:\n",
      "\n",
      "class Variable(torch._C._LegacyVariableBase)\n",
      " |  Method resolution order:\n",
      " |      Variable\n",
      " |      torch._C._LegacyVariableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C._LegacyVariableBase:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function tensor:\n",
      "\n",
      "tensor(...)\n",
      "    tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Constructs a tensor with :attr:`data`.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n",
      "        ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
      "        or :func:`torch.Tensor.detach`.\n",
      "        If you have a NumPy ``ndarray`` and want to avoid a copy, use\n",
      "        :func:`torch.from_numpy`.\n",
      "    \n",
      "    Args:\n",
      "        data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
      "            NumPy ``ndarray``, scalar, and other types.\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if None, infers data type from :attr:`data`.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if None, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
      "        tensor([[ 0.1000,  1.2000],\n",
      "                [ 2.2000,  3.1000],\n",
      "                [ 4.9000,  5.2000]])\n",
      "    \n",
      "        >>> torch.tensor([0, 1])  # Type inference on data\n",
      "        tensor([ 0,  1])\n",
      "    \n",
      "        >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
      "                         dtype=torch.float64,\n",
      "                         device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\n",
      "        tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
      "    \n",
      "        >>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\n",
      "        tensor(3.1416)\n",
      "    \n",
      "        >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
      "        tensor([])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.8081,   6.4650,  21.8193])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=Variable(torch.tensor([1,2,3]))\n",
    "x=Variable(torch.tensor([1.,2.,3.]),requires_grad=True)\n",
    "\n",
    "z=torch.norm(4*x*x)\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||x||_{p} = \\sqrt[p]{x_{1}^{p} + x_{2}^{p} + \\ldots + x_{N}^{p}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
