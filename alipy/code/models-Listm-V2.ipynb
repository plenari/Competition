{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. lstm\n",
    "* 用pytorch做listm模型\n",
    "* 将长度变化的句子输入lism 得到隐藏层和输出成，当做编码的结果。\n",
    "* 如果效果不好的话可以试一下，crf?attention?\n",
    "        \n",
    "# 2.  现在的问题。\n",
    "\n",
    "* 预测的时候还要重新计算词向量\n",
    "* 加入embeding层\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "from batch_size import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import pre_train\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import config as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_word=pre_train()\n",
    "data=load_word.load_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 word_embeding\n",
    "如果需要自己训练embeding\n",
    "如果内存真的放不下，最好用embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_chinese(uchar):         \n",
    "    if '\\u4e00' <= uchar<='\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def jiebaclearText(data,stopwords_path=r'../data/stop_words.txt'):\n",
    "    '''保留所有的汉字。\n",
    "    '''\n",
    "    for sents in data['q1']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "    for sents in data['q2']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "                \n",
    "    return data\n",
    "data=jiebaclearText(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['q1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2获得词汇\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab():\n",
    "    vocab=set()\n",
    "    for i in data['q1']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    for i in data['q2']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    return list(vocab)\n",
    "vocab=get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={i:index for index,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word={index:i for index,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.构建Ngarma模型-----不需要\n",
    "     输入词汇的个数，编码后的维度，以及每个x有几个单词\n",
    "     不需要，我可以直接把embeding和lstm合并"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class Ngama(nn.Module):\n",
    "    '''\n",
    "    支持batchsize的Ngama模型\n",
    "    '''\n",
    "    def __init__(self,vocab_size,word_dim,x_len,batch_size=1):\n",
    "        super(Ngama,self).__init__()\n",
    "        self.embeding=nn.Embedding(vocab_size,word_dim)\n",
    "        self.linear1=nn.Linear(word_dim*x_len,128)\n",
    "        self.linear2=nn.Linear(128,vocab_size)\n",
    "        self.batch_size=batch_size\n",
    "    def forward(self,x):\n",
    "        \n",
    "        embeds=self.embeding(x).view(self.batch_size,-1)\n",
    "        out=F.relu(self.linear1(embeds))\n",
    "        out=self.linear2(out)\n",
    "        log_probs=F.log_softmax(out,dim=1)\n",
    "        return log_probs\n",
    "        \n",
    "word_dim=32\n",
    "losses=[]\n",
    "loss_fn=nn.NLLLoss()#超级多的分类问题\n",
    "model=Ngama(len(vocab),word_dim,x_len)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs=1\n",
    "for epoch in range(epochs):\n",
    "    total_loss=torch.tensor([0.])\n",
    "    for i,words in enumerate(train):\n",
    "        x_idx,y_idx=words2index(words)\n",
    "        model.zero_grad()\n",
    "        y_pred=model(torch.autograd.Variable(torch.tensor(x_idx)))\n",
    "        loss=loss_fn(y_pred,torch.autograd.Variable(torch.LongTensor([y_idx])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()    \n",
    "        if i%5000==0:\n",
    "            print(i,loss.item())\n",
    "    losses.append(total_loss)\n",
    "print(losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 数据获取\n",
    "* 获得所有word2vec的矢量\n",
    "* 首先知道需要的矩阵大小初始化一个zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'花呗' in word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 给我一个索引我返回该索引对应的两个句子叠加后的索引和目标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_lstm(idx):\n",
    "    '''\n",
    "    就是把数据传入,batch_size=1\n",
    "    seq_len ,word_dim\n",
    "    '''\n",
    "    idx=idx[0]\n",
    "    words=data['q1'][idx]+data['q2'][idx]\n",
    "    batch=np.zeros((len(words)))\n",
    "    for i,w in enumerate(words):\n",
    "        if w in word2index:\n",
    "            batch[i]=word2index[w]\n",
    "        else:\n",
    "            batch[i]=-1\n",
    "    target=np.array([data['target'][idx]])    \n",
    "    return batch,target \n",
    "x,y=get_data_lstm([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lstm得到隐藏层\n",
    "\n",
    "* 我希望把每个句子（seq_len）内单词对应的矢量按照顺序输入之后得到一个维度相同的隐藏层。\n",
    "* 有两种情况，\n",
    "    * 我把两个句子合并输入，直接得到x,y\n",
    "    * 我把两个句子输入之后得到输出，然后在把输出合并，得到x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 第一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lstm_embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y\n",
    "        \n",
    "        '''\n",
    "        super(lstm_embedding,self).__init__()\n",
    "        self.embd=nn.Embedding(vocab_size,word_dim)\n",
    "        self.lstm=nn.LSTM(word_dim,hidden_dim)\n",
    "        self.hidden2target=nn.Linear(hidden_dim,target_size)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.hidden=self.init_hidden()\n",
    "        self.word_dim=word_dim\n",
    "        self.vocab_size=vocab_size\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    def forward(self,x):\n",
    "        #seq,batch,hidden\n",
    "        x=self.embd(x)\n",
    "        output,self.hidden=self.lstm(x.view(len(x),self.batch_size,self.word_dim),self.hidden)\n",
    "        out=self.hidden2target(self.hidden[0].view(-1,self.hidden_dim))\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.word_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim=128\n",
    "target_size=2\n",
    "model = lstm_embedding(len(vocab),c.word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs=1\n",
    "for epoch in range(epochs):\n",
    "    '''\n",
    "    重新采样\n",
    "    '''\n",
    "    model.train()\n",
    "    dataset=datasets(sample=True)\n",
    "    datas=DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "    total_loss=torch.tensor(0.0)\n",
    "    for i,idx in enumerate(datas):\n",
    "        '''\n",
    "        把两个序列链接起来。\n",
    "        '''\n",
    "        train_x,train_y=get_data_lstm(idx.numpy())\n",
    "        inputs=Variable(torch.LongTensor(train_x))\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "        #moxing\n",
    "        model.zero_grad()\n",
    "        model.hidden=model.init_hidden()\n",
    "        # Step 3. 前向传播\n",
    "        tag_scores = model(inputs)\n",
    "        # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "        loss = loss_function(tag_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        if i%200==0:\n",
    "            print(epoch,i,total_loss/200)\n",
    "            total_loss=torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../model/lstm_v2_100_state_ditc.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred0=[]\n",
    "y=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(data['index'])):\n",
    "        train_x,train_y=get_data_lstm([i])\n",
    "        inputs=Variable(torch.LongTensor(train_x))\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "        y_pred0.append(model(inputs))\n",
    "        y.append(labels)   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.04639271104499815\n"
     ]
    }
   ],
   "source": [
    "y_pred=[i.argmax() for i in y_pred0]\n",
    "\n",
    "print('f1_score',f1_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.30736468500443653\n"
     ]
    }
   ],
   "source": [
    "y_pred=[i.argmin() for i in y_pred0]\n",
    "\n",
    "print('f1_score',f1_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model,'../model/lstm_v2-100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=torch.load('./model/lstm_v2-100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../model/lstm_v2_100_state_ditc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种\n",
    " * 把两个的输出链接起来然后再预测\n",
    " * 还没有修改完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lstm_embedding_cat(nn.Module):\n",
    "    '''\n",
    "    把两个句子的隐藏变量结合之后用线性函数预测。\n",
    "    '''\n",
    "    def __init__(self,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y        \n",
    "        '''\n",
    "        super(lstm_embedding_cat,self).__init__()\n",
    "        #参数\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.word_dim=word_dim\n",
    "        \n",
    "        #第一个句子        \n",
    "        self.lstm_0=nn.LSTM(word_dim,hidden_dim)        \n",
    "        #第二个句子\n",
    "        self.lstm_1=nn.LSTM(word_dim,hidden_dim)       \n",
    "        \n",
    "        #合并线性\n",
    "        self.linear=nn.Linear(2*hidden_dim,2)\n",
    "        \n",
    "        #初始化隐藏过程\n",
    "        self.hidden_0=self.init_hidden()        \n",
    "        self.hidden_1=self.init_hidden()        \n",
    "    def init_hidden(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    def forward(self,x0,x1):\n",
    "        #seq,batch,hidden\n",
    "        #第一个\n",
    "        _,self.hidden_0=self.lstm_0(x0.view(len(x0),self.batch_size,self.word_dim),self.hidden_0)  \n",
    "            \n",
    "        #第二个\n",
    "        _,self.hidden_1=self.lstm_1(x1.view(len(x1),self.batch_size,self.word_dim),self.hidden_1)  \n",
    "\n",
    "        \n",
    "        #合并\n",
    "        x_0=self.hidden_0[0].view(-1,self.hidden_dim)\n",
    "        x_1=self.hidden_1[0].view(-1,self.hidden_dim)\n",
    "        x_cat=torch.cat([x_0,x_1],dim=1)\n",
    "        out=self.linear(x_cat)\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss vs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dim=32\n",
    "hidden_dim=24\n",
    "target_size=2\n",
    "model = lstm_embedding_cat(word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "seq_len0=np.random.randint(10)+5\n",
    "seq_len1=np.random.randint(10)+5\n",
    "for i in range(10):   \n",
    "    \n",
    "    x0=torch.randn((seq_len0,word_dim))\n",
    "    x1=torch.randn((seq_len1,word_dim))\n",
    "    y=torch.LongTensor(torch.randint(low=0,high=2,size=(1,)).long())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.hidden_0=model.init_hidden()\n",
    "    model.hidden_1=model.init_hidden()\n",
    "    # Step 3. 前向传播\n",
    "    tag_scores = model(x0,x1)\n",
    "    # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "    loss = loss_function(tag_scores, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.欠采样"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "#X_resampled, y_resampled = rus.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
