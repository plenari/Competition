{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. lstm\n",
    "* 用pytorch做listm模型\n",
    "* 将长度变化的句子输入lism 得到隐藏层和输出成，当做编码的结果。\n",
    "* 如果效果不好的话可以试一下，crf?attention?\n",
    "        \n",
    "# 2.  现在的问题。\n",
    "\n",
    "* 预测的时候还要重新计算词向量\n",
    "* 加入embeding层\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "from batch_size import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import pre_train\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import config as c\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_word=pre_train()\n",
    "data=load_word.load_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 word_embeding\n",
    "如果需要自己训练embeding\n",
    "如果内存真的放不下，最好用embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_chinese(uchar):         \n",
    "    if '\\u4e00' <= uchar<='\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def jiebaclearText(data,stopwords_path=r'../data/stop_words.txt'):\n",
    "    '''保留所有的汉字。\n",
    "    '''\n",
    "    for sents in data['q1']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "    for sents in data['q2']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "                \n",
    "    return data\n",
    "data=jiebaclearText(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['怎么', '更改', '花', '呗', '手机号码']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2获得词汇\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab():\n",
    "    vocab=set()\n",
    "    for i in data['q1']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    for i in data['q2']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    return list(vocab)\n",
    "vocab=get_vocab()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 只能读取，除非我又改了切词的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2index=pickle.load(open('../data/word2index-3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vocab to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'花呗' in word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={i:index for index,i in enumerate(vocab)}\n",
    "pickle.dump(word2index,open('word2index-2.pkl','wb'),2)\n",
    "pickle.dump(word2index,open('word2index-3.pkl','wb'),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lstm得到隐藏层\n",
    "\n",
    "* 我希望把每个句子（seq_len）内单词对应的矢量按照顺序输入之后得到一个维度相同的隐藏层。\n",
    "* 有两种情况，\n",
    "    * 我把两个句子合并输入，直接得到x,y\n",
    "    * 我把两个句子输入之后得到输出，然后在把输出合并，得到x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../model/lstm_v2_100_state_ditc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../model/lstm_v2_100_state_ditc.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种\n",
    " * 把两个的输出链接起来然后再预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feed data\n",
    "def get_one_word_for_concat(word):\n",
    "    words=np.zeros((len(word),))-1\n",
    "    for i,w in enumerate(word):\n",
    "        if w in word2index:\n",
    "            words[i]=word2index[w]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feed data2\n",
    "def get_data_lstm_concat(idx):\n",
    "    '''\n",
    "    就是把数据传入,batch_size=1\n",
    "    seq_len ,word_dim\n",
    "    '''\n",
    "    idx=idx[0]\n",
    "    word1=get_one_word_for_concat(data['q1'][idx])\n",
    "    word2=get_one_word_for_concat(data['q2'][idx])\n",
    "    target=np.array([data['target'][idx]])    \n",
    "    return word1,word2,target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11696., 11322., 11622., 11342.,  4697.,  7137., 13114.,  6344.,\n",
       "         5996.,  6344.]),\n",
       " array([10902.,  3022.,  3639., 11342.,  4697.,  6240.]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0,x1,y=get_data_lstm_concat([1])\n",
    "x0,x1,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lstm_embedding_concat(nn.Module):\n",
    "    '''\n",
    "    把两个句子的隐藏变量结合之后用线性函数预测。\n",
    "    '''\n",
    "    def __init__(self,vocab_size,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y        \n",
    "        '''\n",
    "        super(lstm_embedding_concat,self).__init__()\n",
    "        #参数\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.word_dim=word_dim\n",
    "        \n",
    "        self.embd=nn.Embedding(vocab_size,word_dim)\n",
    "        #第一个句子        \n",
    "        self.lstm_0=nn.LSTM(word_dim,hidden_dim,num_layers=2,dropout=0.3,bidirectional=True)\n",
    "        #第二个句子\n",
    "        self.lstm_1=nn.LSTM(word_dim,hidden_dim,num_layers=2,dropout=0.3,bidirectional=True)\n",
    "        \n",
    "        #合并线性\n",
    "        self.linear=nn.Linear(2*(self.lstm_1.hidden_size)*(self.lstm_1.num_layers)*(self.lstm_1.bidirectional+1),64)\n",
    "        self.linear2=nn.Linear(64,2)\n",
    "        #初始化隐藏过程\n",
    "        self.hidden_0=self.init_hidden_0()        \n",
    "        self.hidden_1=self.init_hidden_1()     \n",
    "        \n",
    "    def init_hidden_0(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(self.lstm_0.num_layers*(self.lstm_0.bidirectional+1), self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(self.lstm_0.num_layers*(self.lstm_0.bidirectional+1), self.batch_size, self.hidden_dim)))  \n",
    "    def init_hidden_1(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(self.lstm_1.num_layers*(self.lstm_1.bidirectional+1), self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(self.lstm_1.num_layers*(self.lstm_1.bidirectional+1), self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    \n",
    "    def forward(self,x0,x1):\n",
    "        #seq,batch,hidden\n",
    "        #第一个        \n",
    "        '''\n",
    "        input:seq_len, batch, input_size\n",
    "        output:(seq_len, batch, hidden_size * num_directions)\n",
    "        H_0:(num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        '''\n",
    "        x0=self.embd(x0)\n",
    "        x1=self.embd(x1)\n",
    "        _,self.hidden_0=self.lstm_0(x0.view(len(x0),self.batch_size,self.word_dim),self.hidden_0)  \n",
    "            \n",
    "        #第二个\n",
    "        _,self.hidden_1=self.lstm_1(x1.view(len(x1),self.batch_size,self.word_dim),self.hidden_1)  \n",
    "\n",
    "        \n",
    "        #合并\n",
    "        x_0=self.hidden_0[0].view(-1,(self.lstm_0.hidden_size)*(self.lstm_0.num_layers)*(self.lstm_0.bidirectional+1))\n",
    "        x_1=self.hidden_1[0].view(-1,(self.lstm_1.hidden_size)*(self.lstm_1.num_layers)*(self.lstm_1.bidirectional+1))\n",
    "        \n",
    "        x_cat=torch.cat([x_0,x_1],dim=1)\n",
    "        out=F.relu(self.linear(x_cat))\n",
    "        out=F.relu(self.linear2(out))\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dim=64\n",
    "hidden_dim=128\n",
    "target_size=2\n",
    "model = lstm_embedding_concat(len(word2index),word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6931, -0.6931]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n",
      "tensor([[-0.6931, -0.6931]])\n",
      "0.6931471824645996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-9a5c82ff8e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Step 3. 前向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-a608c8630145>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x0, x1)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m#第二个\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mnexth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[1;31m# hack to handle LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mgates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mingate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amax/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "for epoch in range(epochs):\n",
    "    '''\n",
    "    重新采样\n",
    "    '''\n",
    "    model.train()\n",
    "    dataset=datasets(sample=True)\n",
    "    datas=DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "    total_loss=torch.tensor(0.0)\n",
    "    for i,idx in enumerate(datas):\n",
    "        '''\n",
    "        把两个序列链接起来。\n",
    "        '''\n",
    "        x0,x1,train_y=get_data_lstm_concat(idx.numpy())\n",
    "\n",
    "        x0=Variable(torch.LongTensor(x0))\n",
    "        x1=Variable(torch.LongTensor(x1))\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "       \n",
    "\n",
    "        model.zero_grad()\n",
    "        model.hidden_0=model.init_hidden_0()\n",
    "        model.hidden_1=model.init_hidden_1()\n",
    "        # Step 3. 前向传播\n",
    "        tag_scores = model(x0,x1)       \n",
    "        print(tag_scores)\n",
    "        # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "        loss = loss_function(tag_scores, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        if i%20000==0 and i!=0:\n",
    "            print(epoch,i,total_loss/20000)\n",
    "            total_loss=torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred0=[]\n",
    "y=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(data['index'])):\n",
    "        train_x,train_y=get_data_lstm([i])\n",
    "        inputs=Variable(torch.LongTensor(train_x))\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "        y_pred0.append(model(inputs))\n",
    "        y.append(labels)    \n",
    "        \n",
    "y_pred=[i.argmax() for i in y_pred]\n",
    "print('f1_score',f1_score(y,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
