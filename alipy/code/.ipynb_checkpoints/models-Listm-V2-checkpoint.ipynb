{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. lstm\n",
    "* 用pytorch做listm模型\n",
    "* 将长度变化的句子输入lism 得到隐藏层和输出成，当做编码的结果。\n",
    "* 如果效果不好的话可以试一下，crf?attention?\n",
    "        \n",
    "# 2.  现在的问题。\n",
    "\n",
    "* 预测的时候还要重新计算词向量\n",
    "* 加入embeding层\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\app\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "import gensim\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from batch_size import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import pre_train\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import config as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_model=gensim.models.KeyedVectors.load_word2vec_format(c.word2vec_file,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word=pre_train()\n",
    "data=load_word.load_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 word_embeding\n",
    "如果需要自己训练embeding\n",
    "如果内存真的放不下，最好用embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):         \n",
    "    if '\\u4e00' <= uchar<='\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def jiebaclearText(data,stopwords_path=r'../data/stop_words.txt'):\n",
    "    '''保留所有的汉字。\n",
    "    '''\n",
    "    for sents in data['q1']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "    for sents in data['q2']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "                \n",
    "    return data\n",
    "data=jiebaclearText(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['怎么', '更改', '花', '呗', '手机号码']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2获得词汇\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab():\n",
    "    vocab=set()\n",
    "    for i in data['q1']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    for i in data['q2']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    return list(vocab)\n",
    "vocab=get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={i:index for index,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word={index:i for index,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'能'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words2index(words):\n",
    "    '''\n",
    "    包含x和y\n",
    "    '''\n",
    "    x,y=words\n",
    "    x=[word2index[i] for i in x]\n",
    "    return x,word2index[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.构建Ngarma模型-----不需要\n",
    "     输入词汇的个数，编码后的维度，以及每个x有几个单词\n",
    "     不需要，我可以直接把embeding和lstm合并"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class Ngama(nn.Module):\n",
    "    '''\n",
    "    支持batchsize的Ngama模型\n",
    "    '''\n",
    "    def __init__(self,vocab_size,word_dim,x_len,batch_size=1):\n",
    "        super(Ngama,self).__init__()\n",
    "        self.embeding=nn.Embedding(vocab_size,word_dim)\n",
    "        self.linear1=nn.Linear(word_dim*x_len,128)\n",
    "        self.linear2=nn.Linear(128,vocab_size)\n",
    "        self.batch_size=batch_size\n",
    "    def forward(self,x):\n",
    "        \n",
    "        embeds=self.embeding(x).view(self.batch_size,-1)\n",
    "        out=F.relu(self.linear1(embeds))\n",
    "        out=self.linear2(out)\n",
    "        log_probs=F.log_softmax(out,dim=1)\n",
    "        return log_probs\n",
    "        \n",
    "word_dim=32\n",
    "losses=[]\n",
    "loss_fn=nn.NLLLoss()#超级多的分类问题\n",
    "model=Ngama(len(vocab),word_dim,x_len)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs=1\n",
    "for epoch in range(epochs):\n",
    "    total_loss=torch.tensor([0.])\n",
    "    for i,words in enumerate(train):\n",
    "        x_idx,y_idx=words2index(words)\n",
    "        model.zero_grad()\n",
    "        y_pred=model(torch.autograd.Variable(torch.tensor(x_idx)))\n",
    "        loss=loss_fn(y_pred,torch.autograd.Variable(torch.LongTensor([y_idx])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()    \n",
    "        if i%5000==0:\n",
    "            print(i,loss.item())\n",
    "    losses.append(total_loss)\n",
    "print(losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 数据获取\n",
    "* 获得所有word2vec的矢量\n",
    "* 首先知道需要的矩阵大小初始化一个zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'花呗' in word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 给我一个索引我返回该索引对应的两个句子叠加后的索引和目标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_lstm(idx):\n",
    "    '''\n",
    "    就是把数据传入,batch_size=1\n",
    "    seq_len ,word_dim\n",
    "    '''\n",
    "    idx=idx[0]\n",
    "    words=data['q1'][idx]+data['q2'][idx]\n",
    "    batch=np.zeros((len(words)))\n",
    "    for i,w in enumerate(words):\n",
    "        if w in word2index:\n",
    "            batch[i]=word2index[w]\n",
    "        else:\n",
    "            batch[i]=-1\n",
    "    target=np.array([data['target'][idx]])    \n",
    "    return batch,target \n",
    "x,y=get_data_lstm([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4742.,  6260.,  2989.,  8557.,  2918.,   763., 11211.,  2989.,\n",
       "        8557.,  8330.,  2551., 11211.,  2918.,  4742., 12279.,  4600.,\n",
       "        8522., 11211.,  2006., 11211.,  6352.,  1317.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lstm得到隐藏层\n",
    "\n",
    "* 我希望把每个句子（seq_len）内单词对应的矢量按照顺序输入之后得到一个维度相同的隐藏层。\n",
    "* 有两种情况，\n",
    "    * 我把两个句子合并输入，直接得到x,y\n",
    "    * 我把两个句子输入之后得到输出，然后在把输出合并，得到x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 第一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y\n",
    "        \n",
    "        '''\n",
    "        super(lstm_embedding,self).__init__()\n",
    "        self.embd=nn.Embedding(vocab_size,word_dim)\n",
    "        self.lstm=nn.LSTM(word_dim,hidden_dim)\n",
    "        self.hidden2target=nn.Linear(hidden_dim,target_size)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.hidden=self.init_hidden()\n",
    "        self.word_dim=word_dim\n",
    "        self.vocab_size=vocab_size\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    def forward(self,x):\n",
    "        #seq,batch,hidden\n",
    "        x=self.embd(x)\n",
    "        output,self.hidden=self.lstm(x.view(len(x),self.batch_size,self.word_dim),self.hidden)\n",
    "        out=self.hidden2target(self.hidden[0].view(-1,self.hidden_dim))\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim=128\n",
    "target_size=2\n",
    "model = lstm_embedding(len(vocab),c.word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(0.6208)\n",
      "0 2000 tensor(1384.8494)\n",
      "0 4000 tensor(1372.8821)\n",
      "0 6000 tensor(1377.3729)\n",
      "0 8000 tensor(1365.9843)\n",
      "0 10000 tensor(1381.2008)\n",
      "0 12000 tensor(1378.2716)\n",
      "0 14000 tensor(1371.9218)\n",
      "0 16000 tensor(1372.0477)\n",
      "0 18000 tensor(1378.0193)\n",
      "0 20000 tensor(1367.0454)\n",
      "0 22000 tensor(1372.9286)\n",
      "0 24000 tensor(1371.8091)\n",
      "0 26000 tensor(1368.1826)\n",
      "0 28000 tensor(1366.9783)\n",
      "0 30000 tensor(1366.0089)\n",
      "0 32000 tensor(1374.4110)\n",
      "0 34000 tensor(1374.4071)\n",
      "0 36000 tensor(1362.1024)\n",
      "1 0 tensor(0.3830)\n",
      "1 2000 tensor(1364.2457)\n",
      "1 4000 tensor(1335.5741)\n",
      "1 6000 tensor(1361.5715)\n",
      "1 8000 tensor(1349.9744)\n",
      "1 10000 tensor(1365.8562)\n",
      "1 12000 tensor(1353.6155)\n",
      "1 14000 tensor(1359.9366)\n",
      "1 16000 tensor(1336.3777)\n",
      "1 18000 tensor(1382.5422)\n",
      "1 20000 tensor(1367.7896)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-ee6ee18624b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\app\\anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\app\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    '''\n",
    "    重新采样\n",
    "    '''\n",
    "    dataset=datasets(sample=True)\n",
    "    datas=DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "    total_loss=torch.tensor(0.0)\n",
    "    for i,idx in enumerate(datas):\n",
    "        '''\n",
    "        把两个序列链接起来。\n",
    "        '''\n",
    "        train_x,train_y=get_data_lstm(idx.numpy())\n",
    "        inputs=Variable(torch.LongTensor(train_x))\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "        #moxing\n",
    "        model.zero_grad()\n",
    "        model.hidden=model.init_hidden()\n",
    "        # Step 3. 前向传播\n",
    "        tag_scores = model(inputs)\n",
    "        # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "        loss = loss_function(tag_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "        if i%2000==0:\n",
    "            print(epoch,i,total_loss/2000)\n",
    "            total_loss=torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种\n",
    " * 把两个的输出链接起来然后再预测\n",
    " * 还没有修改完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lstm_embedding_cat(nn.Module):\n",
    "    '''\n",
    "    把两个句子的隐藏变量结合之后用线性函数预测。\n",
    "    '''\n",
    "    def __init__(self,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y        \n",
    "        '''\n",
    "        super(lstm_embedding_cat,self).__init__()\n",
    "        #参数\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.word_dim=word_dim\n",
    "        \n",
    "        #第一个句子        \n",
    "        self.lstm_0=nn.LSTM(word_dim,hidden_dim)        \n",
    "        #第二个句子\n",
    "        self.lstm_1=nn.LSTM(word_dim,hidden_dim)       \n",
    "        \n",
    "        #合并线性\n",
    "        self.linear=nn.Linear(2*hidden_dim,2)\n",
    "        \n",
    "        #初始化隐藏过程\n",
    "        self.hidden_0=self.init_hidden()        \n",
    "        self.hidden_1=self.init_hidden()        \n",
    "    def init_hidden(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    def forward(self,x0,x1):\n",
    "        #seq,batch,hidden\n",
    "        #第一个\n",
    "        _,self.hidden_0=self.lstm_0(x0.view(len(x0),self.batch_size,self.word_dim),self.hidden_0)  \n",
    "            \n",
    "        #第二个\n",
    "        _,self.hidden_1=self.lstm_1(x1.view(len(x1),self.batch_size,self.word_dim),self.hidden_1)  \n",
    "\n",
    "        \n",
    "        #合并\n",
    "        x_0=self.hidden_0[0].view(-1,self.hidden_dim)\n",
    "        x_1=self.hidden_1[0].view(-1,self.hidden_dim)\n",
    "        x_cat=torch.cat([x_0,x_1],dim=1)\n",
    "        out=self.linear(x_cat)\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss vs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dim=32\n",
    "hidden_dim=24\n",
    "target_size=2\n",
    "model = lstm_embedding_cat(word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "seq_len0=np.random.randint(10)+5\n",
    "seq_len1=np.random.randint(10)+5\n",
    "for i in range(10):   \n",
    "    \n",
    "    x0=torch.randn((seq_len0,word_dim))\n",
    "    x1=torch.randn((seq_len1,word_dim))\n",
    "    y=torch.LongTensor(torch.randint(low=0,high=2,size=(1,)).long())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.hidden_0=model.init_hidden()\n",
    "    model.hidden_1=model.init_hidden()\n",
    "    # Step 3. 前向传播\n",
    "    tag_scores = model(x0,x1)\n",
    "    # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "    loss = loss_function(tag_scores, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Bi-LSTM CRF\n",
    "* 作出动态决策和 Bi-LSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y=get_data_cnn(list(np.arange(len(datas['index']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y=model(inputs)\n",
    "\n",
    "y_pred=y.argmax(dim=1).numpy()    \n",
    "\n",
    "print('f1_score',i,f1_score(train_y,y_pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练的结果导致需要一个阈值来保证最大化\n",
    "        如何求阈值？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 将两列合并之后再训练\n",
    "   \n",
    "       1.1 lightgbm 简单\n",
    "       1.2 线性呢？\n",
    "       1.3 torch神经网络？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.欠采样"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "#X_resampled, y_resampled = rus.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
