{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. lstm\n",
    "* 用pytorch做listm模型\n",
    "* 将长度变化的句子输入lism 得到隐藏层和输出成，当做编码的结果。\n",
    "* 如果效果不好的话可以试一下，crf?attention?\n",
    "        \n",
    "# 2.  现在的问题。\n",
    "* 之前用的别人的词向量训练的不好，自己做的那个词太少训练的也不行。\n",
    "* 更换词向量，重写采样的程序，load程序。\n",
    "* 预测的时候还要重新计算词向量\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\app\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "import gensim\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from batch_size import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import pre_train\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import config as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=gensim.models.KeyedVectors.load_word2vec_format(c.word2vec_file,binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word=pre_train()\n",
    "data=load_word.load_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. word_embeding\n",
    "如果需要自己训练embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=set()\n",
    "for i in data['q1']:\n",
    "    for j in i:\n",
    "        vocab.add(j)\n",
    "for i in data['q2']:\n",
    "    for j in i:\n",
    "        vocab.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'。' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={i:index for index,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word={index:i for index,i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2index(words):\n",
    "    '''\n",
    "    包含x和y\n",
    "    '''\n",
    "    x,y=words\n",
    "    x=[word2index[i] for i in x]\n",
    "    return x,word2index[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=[]\n",
    "for i in data['q1']:\n",
    "    string.extend(i)\n",
    "    if i[-1].strip()!='。':\n",
    "        string.append('。')\n",
    "\n",
    "for i in data['q2']:\n",
    "    string.extend(i)\n",
    "    if i[-1].strip()!='。':\n",
    "        string.append('。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把字符分成xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len=3\n",
    "train=[(string[i:i+x_len],string[i+x_len]) for  i in range(len(string)-x_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.构建Ngarma模型\n",
    "     输入词汇的个数，编码后的维度，以及每个x有几个单词\n",
    "     不需要，我可以直接把embeding和lstm合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngama(nn.Module):\n",
    "    '''\n",
    "    支持batchsize的Ngama模型\n",
    "    '''\n",
    "    def __init__(self,vocab_size,word_dim,x_len,batch_size=1):\n",
    "        super(Ngama,self).__init__()\n",
    "        self.embeding=nn.Embedding(vocab_size,word_dim)\n",
    "        self.linear1=nn.Linear(word_dim*x_len,128)\n",
    "        self.linear2=nn.Linear(128,vocab_size)\n",
    "        self.batch_size=batch_size\n",
    "    def forward(self,x):\n",
    "        \n",
    "        embeds=self.embeding(x).view(self.batch_size,-1)\n",
    "        out=F.relu(self.linear1(embeds))\n",
    "        out=self.linear2(out)\n",
    "        log_probs=F.log_softmax(out,dim=1)\n",
    "        return log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim=32\n",
    "losses=[]\n",
    "loss_fn=nn.NLLLoss()#超级多的分类问题\n",
    "model=Ngama(len(vocab),word_dim,x_len)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "for epoch in range(epochs):\n",
    "    total_loss=torch.tensor([0.])\n",
    "    for i,words in enumerate(train):\n",
    "        x_idx,y_idx=words2index(words)\n",
    "        model.zero_grad()\n",
    "        y_pred=model(torch.autograd.Variable(torch.tensor(x_idx)))\n",
    "        loss=loss_fn(y_pred,torch.autograd.Variable(torch.LongTensor([y_idx])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()    \n",
    "        if i%5000==0:\n",
    "            print(i,loss.item())\n",
    "    losses.append(total_loss)\n",
    "print(losses)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据获取\n",
    "* 获得所有word2vec的矢量\n",
    "* 首先知道需要的矩阵大小初始化一个zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([data['target'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_lstm(idx):\n",
    "    '''\n",
    "    就是把数据传入,batch_size=1\n",
    "    seq_len ,word_dim\n",
    "    '''\n",
    "    idx=idx[0]\n",
    "    words=data['q1'][idx],data['q2'][idx]\n",
    "    batch=np.zeros((len(words),c.word_dim))\n",
    "    for i,w in enumerate(words):\n",
    "        batch[i]=transform_word_vector_lstm(w)        \n",
    "    target=np.array([data['target'][idx]])    \n",
    "    return batch,target \n",
    "\n",
    "def transform_word_vector_lstm(word_lists):\n",
    "    '''\n",
    "    分好词的句子的列表\n",
    "    '''\n",
    "    re=np.zeros((c.max_words,128))\n",
    "    insert_vec=0\n",
    "    for word in word_lists:\n",
    "        if insert_vec<re.shape[0]:\n",
    "            if word in word2vec_model.vocab:\n",
    "                re[insert_vec]=word2vec_model[word]\n",
    "                insert_vec+=1\n",
    "    return re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout2d(p=0.2)\n",
    "input = torch.randn(20, 16, 32, 32)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lstm得到隐藏层\n",
    "\n",
    "* 我希望把每个句子（seq_len）内单词对应的矢量按照顺序输入之后得到一个维度相同的隐藏层。\n",
    "* 有两种情况，\n",
    "    * 我把两个句子合并输入，直接得到x,y\n",
    "    * 我把两个句子输入之后得到输出，然后在把输出合并，得到x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 第一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_embedding(nn.Module):\n",
    "    def __init__(self,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y\n",
    "        \n",
    "        '''\n",
    "        super(lstm_embedding,self).__init__()\n",
    "        self.lstm=nn.LSTM(word_dim,hidden_dim)\n",
    "        self.hidden2target=nn.Linear(hidden_dim,target_size)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.hidden=self.init_hidden()\n",
    "        self.word_dim=word_dim\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    def forward(self,x):\n",
    "        #seq,batch,hidden\n",
    "        output,self.hidden=self.lstm(x.view(len(x),self.batch_size,self.word_dim),self.hidden)\n",
    "        out=self.hidden2target(self.hidden[0].view(-1,self.hidden_dim))\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim=24\n",
    "target_size=2\n",
    "model = lstm_embedding(c.word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6173725724220276\n",
      "0.6674570441246033\n",
      "0.6059485673904419\n",
      "0.6580852270126343\n",
      "0.7584689855575562\n",
      "0.5219730734825134\n",
      "0.5025939345359802\n",
      "0.8760128021240234\n",
      "0.47760769724845886\n",
      "0.6374208331108093\n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    '''\n",
    "    重新采样\n",
    "    '''\n",
    "    dataset=datasets()\n",
    "    datas=DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "    \n",
    "    for i,idx in enumerate(datas):\n",
    "        '''\n",
    "        把两个序列链接起来。\n",
    "        '''\n",
    "        train_x,train_y=get_data_cnn(idx.numpy())\n",
    "        inputs=Variable(torch.from_numpy(train_x).float())\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "        #moxing\n",
    "        model.zero_grad()\n",
    "        model.hidden=model.init_hidden()\n",
    "        # Step 3. 前向传播\n",
    "        tag_scores = model(x)\n",
    "        # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "        loss = loss_function(tag_scores, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种\n",
    " * 把两个的输出链接起来然后再预测\n",
    " * 还没有修改完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_embedding_cat(nn.Module):\n",
    "    '''\n",
    "    把两个句子的隐藏变量结合之后用线性函数预测。\n",
    "    '''\n",
    "    def __init__(self,word_dim,hidden_dim,target_size,batch_size=1,):\n",
    "        '''\n",
    "        输入一个长句子，包含很多词向量seq_len。\n",
    "        得到hidden,然后用hidden做x预测y        \n",
    "        '''\n",
    "        super(lstm_embedding_cat,self).__init__()\n",
    "        #参数\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.word_dim=word_dim\n",
    "        \n",
    "        #第一个句子        \n",
    "        self.lstm_0=nn.LSTM(word_dim,hidden_dim)        \n",
    "        #第二个句子\n",
    "        self.lstm_1=nn.LSTM(word_dim,hidden_dim)       \n",
    "        \n",
    "        #合并线性\n",
    "        self.linear=nn.Linear(2*hidden_dim,2)\n",
    "        \n",
    "        #初始化隐藏过程\n",
    "        self.hidden_0=self.init_hidden()        \n",
    "        self.hidden_1=self.init_hidden()        \n",
    "    def init_hidden(self):\n",
    "        '''两个句子之间没有联系，所有我每个句子都要随机初始化\n",
    "        # 各个维度的含义是 (num_layers, minibatch_size, hidden_dim)\n",
    "        '''     \n",
    "        return (torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim)))  \n",
    "    \n",
    "    def forward(self,x0,x1):\n",
    "        #seq,batch,hidden\n",
    "        #第一个\n",
    "        _,self.hidden_0=self.lstm_0(x0.view(len(x0),self.batch_size,self.word_dim),self.hidden_0)  \n",
    "            \n",
    "        #第二个\n",
    "        _,self.hidden_1=self.lstm_1(x1.view(len(x1),self.batch_size,self.word_dim),self.hidden_1)  \n",
    "\n",
    "        \n",
    "        #合并\n",
    "        x_0=self.hidden_0[0].view(-1,self.hidden_dim)\n",
    "        x_1=self.hidden_1[0].view(-1,self.hidden_dim)\n",
    "        x_cat=torch.cat([x_0,x_1],dim=1)\n",
    "        out=self.linear(x_cat)\n",
    "        out=F.log_softmax(out,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss vs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dim=32\n",
    "hidden_dim=24\n",
    "target_size=2\n",
    "model = lstm_embedding_cat(word_dim, hidden_dim,target_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len0=np.random.randint(10)+5\n",
    "seq_len1=np.random.randint(10)+5\n",
    "for i in range(10):   \n",
    "    \n",
    "    x0=torch.randn((seq_len0,word_dim))\n",
    "    x1=torch.randn((seq_len1,word_dim))\n",
    "    y=torch.LongTensor(torch.randint(low=0,high=2,size=(1,)).long())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.hidden_0=model.init_hidden()\n",
    "    model.hidden_1=model.init_hidden()\n",
    "    # Step 3. 前向传播\n",
    "    tag_scores = model(x0,x1)\n",
    "    # Step 4. 计算损失和梯度值 通过调用 optimizer.step() 来更新梯度\n",
    "    loss = loss_function(tag_scores, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Bi-LSTM CRF\n",
    "* 作出动态决策和 Bi-LSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=get_data_cnn(list(np.arange(len(datas['index']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y=model(inputs)\n",
    "\n",
    "y_pred=y.argmax(dim=1).numpy()    \n",
    "\n",
    "print('f1_score',i,f1_score(train_y,y_pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练的结果导致需要一个阈值来保证最大化\n",
    "        如何求阈值？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 将两列合并之后再训练\n",
    "   \n",
    "       1.1 lightgbm 简单\n",
    "       1.2 线性呢？\n",
    "       1.3 torch神经网络？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.欠采样"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "#X_resampled, y_resampled = rus.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
