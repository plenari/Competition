{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. lstm\n",
    "* 用pytorch做listm模型\n",
    "* 将长度变化的句子输入lism 得到隐藏层和输出成，当做编码的结果。\n",
    "* 如果效果不好的话可以试一下，crf?attention?\n",
    "        \n",
    "# 2.  现在的问题。\n",
    "\n",
    "* 预测的时候还要重新计算词向量\n",
    "* 加入embeding层\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\app\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "from batch_size import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from preprocess import pre_train\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import config as c\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word=pre_train()\n",
    "data=load_word.load_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 word_embeding\n",
    "如果需要自己训练embeding\n",
    "如果内存真的放不下，最好用embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):         \n",
    "    if '\\u4e00' <= uchar<='\\u9fff':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def jiebaclearText(data,stopwords_path=r'../data/stop_words.txt'):\n",
    "    '''保留所有的汉字。\n",
    "    '''\n",
    "    for sents in data['q1']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "    for sents in data['q2']:\n",
    "        '''句子'''\n",
    "        for w in sents:\n",
    "            '''一个单词'''\n",
    "            if not is_chinese(w):\n",
    "                sents.remove(w)\n",
    "                \n",
    "    return data\n",
    "data=jiebaclearText(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['怎么', '更改', '花', '呗', '手机号码']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['q1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2获得词汇\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab():\n",
    "    vocab=set()\n",
    "    for i in data['q1']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    for i in data['q2']:\n",
    "        for j in i:\n",
    "            vocab.add(j)\n",
    "    return list(vocab)\n",
    "vocab=get_vocab()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 只能读取，除非我又改了切词的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index=pickle.load(open('../data/word2index-3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vocab to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'花呗' in word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={i:index for index,i in enumerate(vocab)}\n",
    "pickle.dump(word2index,open('word2index-2.pkl','wb'),2)\n",
    "pickle.dump(word2index,open('word2index-3.pkl','wb'),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lstm得到隐藏层\n",
    "\n",
    "* 我希望把每个句子（seq_len）内单词对应的矢量按照顺序输入之后得到一个维度相同的隐藏层。\n",
    "* 有两种情况，\n",
    "    * 我把两个句子合并输入，直接得到x,y\n",
    "    * 我把两个句子输入之后得到输出，然后在把输出合并，得到x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../model/lstm_v2_100_state_ditc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../model/lstm_v2_100_state_ditc.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二种\n",
    " * 把两个的输出链接起来然后再预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed data\n",
    "def get_one_word_for_concat(word):\n",
    "    words=np.zeros((len(word),))-1\n",
    "    for i,w in enumerate(word):\n",
    "        if w in word2index:\n",
    "            words[i]=word2index[w]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed data2\n",
    "def get_data_lstm_concat(idx):\n",
    "    '''\n",
    "    就是把数据传入,batch_size=1\n",
    "    seq_len ,word_dim\n",
    "    '''\n",
    "    idx=idx[0]\n",
    "    word1=get_one_word_for_concat(data['q1'][idx])\n",
    "    word2=get_one_word_for_concat(data['q2'][idx])\n",
    "    target=np.array([data['target'][idx]])    \n",
    "    return word1,word2,target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11696., 11322., 11622., 11342.,  4697.,  7137., 13114.,  6344.,\n",
       "         5996.,  6344.]),\n",
       " array([10902.,  3022.,  3639., 11342.,  4697.,  6240.]),\n",
       " array([0], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0,x1,y=get_data_lstm_concat([1])\n",
    "x0,x1,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "* Nishant Nikhil (i.nishantnikhil@gmail.com)\n",
    "* An implementation of the Deep Semantic Similarity Model (DSSM) found in [1].\n",
    "* [1] Shen, Y., He, X., Gao, J., Deng, L., and Mesnil, G. 2014. A latent semantic odel with convolutional-pooling structure for information retrieval. In CIKM, pp. 101-110.http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf\n",
    "* [2] http://research.microsoft.com/en-us/projects/dssm/\n",
    "* [3] http://research.microsoft.com/pubs/238873/wsdm2015.v3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_GRAM_SIZE = 3 # See section 3.2.\n",
    "WINDOW_SIZE = 3 # See section 3.2.\n",
    "TOTAL_LETTER_GRAMS = int(3 * 1e4) # Determined from data. See section 3.2.\n",
    "WORD_DEPTH = WINDOW_SIZE * TOTAL_LETTER_GRAMS # See equation (1).\n",
    "# Uncomment it, if testing\n",
    "# WORD_DEPTH = 1000\n",
    "K = 300 # Dimensionality of the max-pooling layer. See section 3.4.\n",
    "L = 128 # Dimensionality of latent semantic space. See section 3.5.\n",
    "J = 4 # Number of random unclicked documents serving as negative examples for a query. See section 4.\n",
    "FILTER_LENGTH = 1 # We only consider one time step for convolutions.\n",
    "\n",
    "\n",
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class CDSSM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CDSSM, self).__init__()\n",
    "        # layers for query\n",
    "        self.query_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)\n",
    "        self.query_sem = nn.Linear(K, L)\n",
    "        # layers for docs\n",
    "        self.doc_conv = nn.Conv1d(WORD_DEPTH, K, FILTER_LENGTH)\n",
    "        self.doc_sem = nn.Linear(K, L)\n",
    "        # learning gamma\n",
    "        self.learn_gamma = nn.Conv1d(1, 1, 1)\n",
    "    def forward(self, q, pos, negs):\n",
    "        # Query model. The paper uses separate neural nets for queries and documents (see section 5.2).\n",
    "        # To make it compatible with Conv layer we reshape it to: (batch_size, WORD_DEPTH, query_len)\n",
    "        q = q.transpose(1,2)\n",
    "        # In this step, we transform each word vector with WORD_DEPTH dimensions into its\n",
    "        # convolved representation with K dimensions. K is the number of kernels/filters\n",
    "        # being used in the operation. Essentially, the operation is taking the dot product\n",
    "        # of a single weight matrix (W_c) with each of the word vectors (l_t) from the\n",
    "        # query matrix (l_Q), adding a bias vector (b_c), and then applying the tanh activation.\n",
    "        # That is, h_Q = tanh(W_c • l_Q + b_c). Note: the paper does not include bias units.\n",
    "        q_c = F.tanh(self.query_conv(q))\n",
    "        # Next, we apply a max-pooling layer to the convolved query matrix.\n",
    "        q_k = kmax_pooling(q_c, 2, 1)\n",
    "        q_k = q_k.transpose(1,2)\n",
    "        # In this step, we generate the semantic vector represenation of the query. This\n",
    "        # is a standard neural network dense layer, i.e., y = tanh(W_s • v + b_s). Again,\n",
    "        # the paper does not include bias units.\n",
    "        q_s = F.tanh(self.query_sem(q_k))\n",
    "        q_s = q_s.resize(L)\n",
    "        # # The document equivalent of the above query model for positive document\n",
    "        pos = pos.transpose(1,2)\n",
    "        pos_c = F.tanh(self.doc_conv(pos))\n",
    "        pos_k = kmax_pooling(pos_c, 2, 1)\n",
    "        pos_k = pos_k.transpose(1,2)\n",
    "        pos_s = F.tanh(self.doc_sem(pos_k))\n",
    "        pos_s = pos_s.resize(L)\n",
    "        # # The document equivalent of the above query model for negative documents\n",
    "        negs = [neg.transpose(1,2) for neg in negs]\n",
    "        neg_cs = [F.tanh(self.doc_conv(neg)) for neg in negs]\n",
    "        neg_ks = [kmax_pooling(neg_c, 2, 1) for neg_c in neg_cs]\n",
    "        neg_ks = [neg_k.transpose(1,2) for neg_k in neg_ks]\n",
    "        neg_ss = [F.tanh(self.doc_sem(neg_k)) for neg_k in neg_ks]\n",
    "        neg_ss = [neg_s.resize(L) for neg_s in neg_ss]\n",
    "        # Now let us calculates the cosine similarity between the semantic representations of\n",
    "        # a queries and documents\n",
    "        # dots[0] is the dot-product for positive document, this is necessary to remember\n",
    "        # because we set the target label accordingly\n",
    "        dots = [q_s.dot(pos_s)]\n",
    "        dots = dots + [q_s.dot(neg_s) for neg_s in neg_ss]\n",
    "        # dots is a list as of now, lets convert it to torch variable\n",
    "        dots = torch.stack(dots)\n",
    "        # In this step, we multiply each dot product value by gamma. In the paper, gamma is\n",
    "        # described as a smoothing factor for the softmax function, and it's set empirically\n",
    "        # on a held-out data set. We're going to learn gamma's value by pretending it's\n",
    "        # a single 1 x 1 kernel.\n",
    "        with_gamma = self.learn_gamma(dots.resize(J+1, 1, 1))\n",
    "        # Finally, we use the softmax function to calculate P(D+|Q).\n",
    "        prob = F.softmax(with_gamma)\n",
    "        return prob\n",
    "\n",
    "model = CDSSM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build a random data set.\n",
    "import numpy as np\n",
    "sample_size = 10\n",
    "l_Qs = []\n",
    "pos_l_Ds = []\n",
    "\n",
    "(query_len, doc_len) = (5, 100)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    query_len = np.random.randint(1, 10)\n",
    "    l_Q = np.random.rand(1, query_len, WORD_DEPTH)\n",
    "    l_Qs.append(l_Q)\n",
    "    \n",
    "    doc_len = np.random.randint(50, 500)\n",
    "    l_D = np.random.rand(1, doc_len, WORD_DEPTH)\n",
    "    pos_l_Ds.append(l_D)\n",
    "\n",
    "neg_l_Ds = [[] for j in range(J)]\n",
    "for i in range(sample_size):\n",
    "    possibilities = list(range(sample_size))\n",
    "    possibilities.remove(i)\n",
    "    negatives = np.random.choice(possibilities, J, replace = False)\n",
    "    for j in range(J):\n",
    "        negative = negatives[j]\n",
    "        neg_l_Ds[j].append(pos_l_Ds[negative])\n",
    "\n",
    "# Till now, we have made a complete numpy dataset\n",
    "# Now let's convert the numpy variables to torch Variable\n",
    "\n",
    "for i in range(len(l_Qs)):\n",
    "    l_Qs[i] = Variable(torch.from_numpy(l_Qs[i]).float())\n",
    "    pos_l_Ds[i] = Variable(torch.from_numpy(pos_l_Ds[i]).float())\n",
    "    for j in range(J):\n",
    "        neg_l_Ds[j][i] = Variable(torch.from_numpy(neg_l_Ds[j][i]).float())\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "# output variable, remember the cosine similarity with positive doc was at 0th index\n",
    "y = np.ndarray(1)\n",
    "# CrossEntropyLoss expects only the index as a long tensor\n",
    "y[0] = 0\n",
    "y = Variable(torch.from_numpy(y).long())\n",
    "\n",
    "for i in range(sample_size):\n",
    "    y_pred = model(l_Qs[i], pos_l_Ds[i], [neg_l_Ds[j][i] for j in range(J)])\n",
    "    loss = criterion(y_pred.resize(1,J+1), y)\n",
    "    print (i, loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred0=[]\n",
    "y=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(data['index'])):\n",
    "        train_x,train_y=get_data_lstm([i])\n",
    "        inputs=Variable(torch.LongTensor(train_x))\n",
    "        labels=Variable(torch.tensor(train_y).long())\n",
    "        y_pred0.append(model(inputs))\n",
    "        y.append(labels)    \n",
    "        \n",
    "y_pred=[i.argmax() for i in y_pred]\n",
    "print('f1_score',f1_score(y,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
